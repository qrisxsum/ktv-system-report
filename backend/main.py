"""
ETL æµç¨‹è”è°ƒè„šæœ¬

åŠŸèƒ½ï¼š
ä¸²è” Parser -> Cleaner å®Œæ•´é“¾è·¯ï¼ŒéªŒè¯æ•°æ®ç»“æ„æ˜¯å¦ç¬¦åˆä¸‹æ¸¸éœ€æ±‚ã€‚
- ä¾› Dev A (æ•°æ®åº“å¼€å‘) éªŒè¯å…¥åº“æ•°æ®æ ¼å¼
- ä¾› Dev C (å‰ç«¯å¼€å‘) éªŒè¯æ ¡éªŒæŠ¥å‘Šæ ¼å¼

ç”¨æ³•ï¼š
    python main.py                                  # å¤„ç†é»˜è®¤æµ‹è¯•æ–‡ä»¶
    python main.py <file1> <file2> ...              # å¤„ç†æŒ‡å®šæ–‡ä»¶
    python main.py --stream <file.csv>              # æµå¼å¤„ç†æ¨¡å¼
    python main.py --stream --chunk-size 2000 <file.csv>  # è‡ªå®šä¹‰ chunk å¤§å°

Author: Dev B (Data Specialist)
Version: 2.0 - æ”¯æŒæµå¼å¤„ç†
"""

import argparse
import json
import sys
import time
import traceback
from datetime import datetime, date
from decimal import Decimal
from pathlib import Path
from typing import Any, List, Tuple, Optional

try:
    import numpy as np
except ImportError:  # pragma: no cover
    np = None

# ============================================================================
# è·¯å¾„è®¾ç½®ï¼šç¡®ä¿èƒ½æ­£ç¡®å¯¼å…¥ app æ¨¡å—
# ============================================================================

# è·å– backend ç›®å½•çš„ç»å¯¹è·¯å¾„
BACKEND_DIR = Path(__file__).resolve().parent
# å°† backend ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
if str(BACKEND_DIR) not in sys.path:
    sys.path.insert(0, str(BACKEND_DIR))

# å¯¼å…¥ ETL æ¨¡å—
from app.services import parser, cleaner
from app.services.cleaner import (
    ETLErrorType,
    CleanerService,
    ValidationResult,
    RowError,
)
from app.services.parser import ParserError, parse_csv_stream, parse_and_validate

# ============================================================================
# å¸¸é‡é…ç½®
# ============================================================================

# å¤§æ–‡ä»¶é˜ˆå€¼ï¼ˆè¶…è¿‡æ­¤å¤§å°è‡ªåŠ¨å¯ç”¨æµå¼å¤„ç†ï¼Œå•ä½ï¼šå­—èŠ‚ï¼‰
LARGE_FILE_THRESHOLD = 20 * 1024 * 1024  # 20MB

# é»˜è®¤æµå¼å¤„ç†çš„ chunk å¤§å°
DEFAULT_CHUNK_SIZE = 5000


# ============================================================================
# JSON åºåˆ—åŒ–è¾…åŠ©å‡½æ•°
# ============================================================================


def json_encoder(obj: Any) -> Any:
    """
    è‡ªå®šä¹‰ JSON åºåˆ—åŒ–å¤„ç†å™¨

    å¤„ç†ä»¥ä¸‹ç±»å‹ï¼š
    - Decimal -> float
    - datetime/date -> ISO 8601 å­—ç¬¦ä¸²
    - bytes -> Base64 å­—ç¬¦ä¸²ï¼ˆæˆ–å¿½ç•¥ï¼‰
    - set -> list

    Args:
        obj: éœ€è¦åºåˆ—åŒ–çš„å¯¹è±¡

    Returns:
        Any: å¯ JSON åºåˆ—åŒ–çš„å€¼
    """
    if isinstance(obj, Decimal):
        return float(obj)
    elif isinstance(obj, datetime):
        return obj.isoformat()
    elif isinstance(obj, date):
        return obj.isoformat()
    elif isinstance(obj, bytes):
        return obj.decode("utf-8", errors="ignore")
    elif isinstance(obj, set):
        return list(obj)
    elif np is not None and isinstance(obj, np.generic):
        # å…¼å®¹ numpy.int64 / numpy.float64 ç­‰ç±»å‹
        return obj.item()
    elif hasattr(obj, "dict"):
        # Pydantic æ¨¡å‹
        return obj.dict()
    elif hasattr(obj, "__dict__"):
        return obj.__dict__
    else:
        raise TypeError(f"æ— æ³•åºåˆ—åŒ–ç±»å‹: {type(obj)}")


def to_json(obj: Any, indent: int = 2) -> str:
    """
    å°†å¯¹è±¡è½¬æ¢ä¸ºç¾åŒ–çš„ JSON å­—ç¬¦ä¸²

    Args:
        obj: éœ€è¦è½¬æ¢çš„å¯¹è±¡
        indent: ç¼©è¿›ç©ºæ ¼æ•°

    Returns:
        str: JSON å­—ç¬¦ä¸²
    """
    return json.dumps(obj, default=json_encoder, ensure_ascii=False, indent=indent)


# ============================================================================
# æŠ¥è¡¨ç±»å‹ä¸­æ–‡åç§°æ˜ å°„
# ============================================================================

REPORT_TYPE_NAMES = {
    "booking": "é¢„è®¢æ±‡æ€»è¡¨",
    "sales": "é…’æ°´é”€å”®åˆ†æè¡¨",
    "room": "åŒ…å¢å¼€å°åˆ†æè¡¨",
    "unknown": "æœªçŸ¥ç±»å‹",
}


# ============================================================================
# ä¸»æµç¨‹å‡½æ•°
# ============================================================================


def process_file_full(file_path: str) -> bool:
    """
    å…¨é‡å¤„ç†å•ä¸ªæ–‡ä»¶çš„å®Œæ•´ ETL æµç¨‹

    æµç¨‹ï¼š
    1. è¯»å–æ–‡ä»¶å†…å®¹
    2. è°ƒç”¨ Parser è§£ææ–‡ä»¶
    3. è°ƒç”¨ Cleaner æ¸…æ´—æ•°æ®å¹¶æ ¡éªŒ
    4. è¾“å‡ºç»“æœæŠ¥å‘Š

    Args:
        file_path: æ–‡ä»¶è·¯å¾„

    Returns:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    print("\n" + "=" * 70)
    print(f"ğŸ“‚ å¤„ç†æ–‡ä»¶: {file_path}")
    print("=" * 70)

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    try:
        # ======== Step 0: è¯»å–æ–‡ä»¶ ========
        file_path_obj = Path(file_path)
        if not file_path_obj.exists():
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
            return False

        filename = file_path_obj.name
        with open(file_path_obj, "rb") as f:
            file_content = f.read()

        print(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå¤§å°: {len(file_content):,} bytes")

        # ======== Step 1: Parse (è§£æ) ========
        print("\n" + "-" * 40)
        print("ã€Step 1ã€‘è§£ææ–‡ä»¶ (Parser)")
        print("-" * 40)

        try:
            df, report_type, meta = parser.parse_and_validate(file_content, filename)
        except parser.ParserError as exc:
            print(f"âŒ è§£æå¤±è´¥: {exc}")
            return False

        print(f"âœ… è§£æå®Œæˆ")
        print(
            f"   - æŠ¥è¡¨ç±»å‹: {report_type} ({REPORT_TYPE_NAMES.get(report_type, 'æœªçŸ¥')})"
        )
        print(f"   - æ•°æ®è¡Œæ•°: {meta['row_count']}")
        print(f"   - åˆ—æ•°: {meta['column_count']}")
        print(f"   - åˆ—ååˆ—è¡¨:")
        for i, col in enumerate(meta["columns"][:10], 1):
            print(f"       {i:2d}. {col}")
        if len(meta["columns"]) > 10:
            print(f"       ... å…± {len(meta['columns'])} åˆ— (ä»…æ˜¾ç¤ºå‰10åˆ—)")

        # ======== Step 2: Clean (æ¸…æ´—) ========
        print("\n" + "-" * 40)
        print("ã€Step 2ã€‘æ¸…æ´—æ•°æ® (Cleaner)")
        print("-" * 40)

        cleaned_data, validation_result = cleaner.clean_and_validate(
            df, report_type, filename=filename, detected_date=meta.get("detected_date")
        )

        # è®¡ç®—è€—æ—¶
        elapsed_time = time.time() - start_time

        print(f"âœ… æ¸…æ´—å®Œæˆ")
        print(f"   - æ¸…æ´—åè¡Œæ•°: {len(cleaned_data)}")
        print(f"   - æ ¡éªŒé€šè¿‡: {'æ˜¯ âœ…' if validation_result.is_valid else 'å¦ âŒ'}")
        print(f"   - é”™è¯¯æ•°é‡: {validation_result.error_count}")

        # ======== è¾“å‡ºç»“æœ (JSON æ ¼å¼) ========
        print("\n" + "-" * 40)
        print("ã€è¾“å‡ºç»“æœã€‘JSON æ ¼å¼æŠ¥å‘Š")
        print("-" * 40)

        # 1. åŸºæœ¬ä¿¡æ¯
        basic_info = {
            "filename": filename,
            "report_type": report_type,
            "report_type_name": REPORT_TYPE_NAMES.get(report_type, "æœªçŸ¥"),
            "elapsed_time_ms": round(elapsed_time * 1000, 2),
            "row_count": len(cleaned_data),
            "column_count": meta["column_count"],
        }

        print("\nğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
        print(to_json(basic_info))

        # 2. æ¸…æ´—æ•°æ®æŠ½æ · (å‰2æ¡)
        sample_data = cleaned_data[:2] if cleaned_data else []

        print("\nğŸ“‹ æ¸…æ´—æ•°æ®æŠ½æ · (å‰2æ¡):")
        print(to_json(sample_data))

        # 3. æ ¡éªŒæŠ¥å‘Š
        validation_report = {
            "is_valid": validation_result.is_valid,
            "summary": validation_result.summary,
            "errors_preview": [
                {
                    "row_index": err.row_index,
                    "column": err.column,
                    "message": err.message,
                    "raw_data": err.raw_data,
                }
                for err in validation_result.errors[:3]  # å‰3ä¸ªé”™è¯¯
            ],
            "total_error_count": validation_result.error_count,
        }

        print("\nğŸ“‹ æ ¡éªŒæŠ¥å‘Š:")
        print(to_json(validation_report))

        # 4. æ•°æ®ç»“æ„è¯´æ˜ (ç»™ä¸‹æ¸¸å¼€å‘è€…å‚è€ƒ)
        if cleaned_data:
            first_record = cleaned_data[0]
            field_types = {}
            for key, value in first_record.items():
                if value is None:
                    field_types[key] = "null"
                elif isinstance(value, dict):
                    field_types[key] = "object (JSON)"
                elif isinstance(value, float):
                    field_types[key] = "number (float)"
                elif isinstance(value, int):
                    field_types[key] = "number (int)"
                elif isinstance(value, str):
                    field_types[key] = "string"
                else:
                    field_types[key] = type(value).__name__

            print("\nğŸ“‹ å­—æ®µç±»å‹è¯´æ˜ (ä¾› Dev A å‚è€ƒ):")
            print(to_json(field_types))

        print("\n" + "-" * 40)
        print(f"â±ï¸ å¤„ç†è€—æ—¶: {elapsed_time * 1000:.2f} ms")
        print("-" * 40)

        return True

    except Exception as e:
        # å¼‚å¸¸å¤„ç†ï¼šæ‰“å°é”™è¯¯å †æ ˆ
        elapsed_time = time.time() - start_time
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        print("\nğŸ“‹ é”™è¯¯å †æ ˆ:")
        traceback.print_exc()
        print(f"\nâ±ï¸ å¤„ç†è€—æ—¶ (å¤±è´¥): {elapsed_time * 1000:.2f} ms")

        return False


def process_file_stream(file_path: str, chunk_size: int = DEFAULT_CHUNK_SIZE) -> bool:
    """
    æµå¼å¤„ç†å•ä¸ª CSV æ–‡ä»¶çš„ ETL æµç¨‹

    é€‚ç”¨äºå¤§æ–‡ä»¶ï¼ˆ>50MBï¼‰çš„ CSV å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡ºã€‚
    é€šè¿‡ç”Ÿæˆå™¨åˆ†å—è¯»å–å’Œå¤„ç†æ•°æ®ã€‚

    æµç¨‹ï¼š
    1. è¯»å–æ–‡ä»¶å†…å®¹
    2. ä½¿ç”¨ parse_csv_stream åˆ†å—è§£æ
    3. å¯¹æ¯ä¸ª chunk è°ƒç”¨ Cleaner æ¸…æ´—
    4. ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯å¹¶è¾“å‡ºæŠ¥å‘Š

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        chunk_size: æ¯ä¸ª chunk çš„è¡Œæ•°ï¼ˆé»˜è®¤ 5000ï¼‰

    Returns:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    print("\n" + "=" * 70)
    print(f"ğŸ“‚ å¤„ç†æ–‡ä»¶: {file_path}")
    print(f"ğŸš€ æ¨¡å¼: æµå¼å¤„ç† (Stream Mode)")
    print(f"ğŸ“¦ Chunk å¤§å°: {chunk_size} è¡Œ")
    print("=" * 70)

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    try:
        # ======== Step 0: è¯»å–æ–‡ä»¶ ========
        file_path_obj = Path(file_path)
        if not file_path_obj.exists():
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
            return False

        # éªŒè¯æ–‡ä»¶ç±»å‹
        filename = file_path_obj.name
        if not filename.lower().endswith(".csv"):
            print(f"âŒ é”™è¯¯: æµå¼å¤„ç†ä»…æ”¯æŒ CSV æ–‡ä»¶ï¼Œå½“å‰æ–‡ä»¶: {filename}")
            print("   ğŸ’¡ æç¤º: è¯·ç§»é™¤ --stream å‚æ•°ä½¿ç”¨å…¨é‡å¤„ç†æ¨¡å¼")
            return False

        with open(file_path_obj, "rb") as f:
            file_content = f.read()

        file_size_mb = len(file_content) / (1024 * 1024)
        print(
            f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå¤§å°: {len(file_content):,} bytes ({file_size_mb:.2f} MB)"
        )

        # ======== Step 1: åˆå§‹åŒ–æœåŠ¡ ========
        cleaner_service = CleanerService()

        # ======== Step 2: è·å–æµå¼ç”Ÿæˆå™¨ ========
        print("\n" + "-" * 40)
        print("ã€å¼€å§‹æµå¼å¤„ç†ã€‘")
        print("-" * 40)

        try:
            stream = parse_csv_stream(file_content, filename, chunk_size=chunk_size)
        except ParserError as exc:
            print(f"âŒ è§£æå¤±è´¥: {exc}")
            return False

        # ======== Step 3: å¾ªç¯å¤„ç†æ¯ä¸ª Chunk ========
        total_rows = 0
        total_cleaned_rows = 0
        all_errors: List[RowError] = []
        all_warnings: List[RowError] = []
        report_type: Optional[str] = None
        chunk_stats: List[dict] = []

        for chunk_idx, (df_chunk, meta) in enumerate(stream):
            chunk_start_time = time.time()

            # è·å–æŠ¥è¡¨ç±»å‹ï¼ˆä»…ç¬¬ä¸€ä¸ª chunkï¼‰
            if report_type is None:
                report_type = meta["report_type"]
                print(
                    f"âœ… æŠ¥è¡¨ç±»å‹: {report_type} ({REPORT_TYPE_NAMES.get(report_type, 'æœªçŸ¥')})"
                )
                print()

            # è°ƒç”¨ Cleaner æ¸…æ´—å½“å‰ Chunk
            cleaned_data, validation_result = cleaner_service.clean_data(
                df_chunk,
                meta["report_type"],
                filename=filename,
                detected_date=meta.get("detected_date"),
            )

            chunk_elapsed = time.time() - chunk_start_time

            # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
            chunk_rows = meta["chunk_rows"]
            total_rows += chunk_rows
            total_cleaned_rows += len(cleaned_data)

            # æ”¶é›†é”™è¯¯å’Œè­¦å‘Šï¼ˆä¿®æ­£è¡Œå·ä¸ºå…¨å±€ç´¢å¼•ï¼‰
            chunk_error_count = 0
            chunk_warning_count = 0
            for error in validation_result.errors:
                # ä¿®æ­£è¡Œå·ï¼šå…¨å±€è¡Œå· = å½“å‰å—ä¹‹å‰çš„è¡Œæ•° + å—å†…è¡Œå·
                if error.row_index != -1:
                    error.row_index += chunk_idx * chunk_size

                # åˆ†ç¦»é”™è¯¯å’Œè­¦å‘Š
                if error.severity == "warning":
                    all_warnings.append(error)
                    chunk_warning_count += 1
                else:
                    all_errors.append(error)
                    chunk_error_count += 1

            # è®°å½• chunk ç»Ÿè®¡
            chunk_stat = {
                "chunk_idx": chunk_idx + 1,
                "rows": chunk_rows,
                "cleaned_rows": len(cleaned_data),
                "errors": chunk_error_count,
                "warnings": chunk_warning_count,
                "elapsed_ms": round(chunk_elapsed * 1000, 2),
            }
            chunk_stats.append(chunk_stat)

            # è¾“å‡º chunk è¿›åº¦
            status_icon = "âœ…" if chunk_error_count == 0 else "âš ï¸"
            print(
                f"  {status_icon} Chunk {chunk_idx + 1}: "
                f"å¤„ç† {chunk_rows} è¡Œ -> {len(cleaned_data)} è¡Œ, "
                f"é”™è¯¯: {chunk_error_count}, è­¦å‘Š: {chunk_warning_count}, "
                f"è€—æ—¶: {chunk_elapsed * 1000:.1f}ms"
            )

        # ======== Step 4: è¾“å‡ºæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š ========
        elapsed_time = time.time() - start_time

        print("\n" + "-" * 40)
        print("ã€æµå¼å¤„ç†å®Œæˆã€‘æ±‡æ€»æŠ¥å‘Š")
        print("-" * 40)

        # åŸºæœ¬ä¿¡æ¯
        summary_info = {
            "filename": filename,
            "mode": "stream",
            "report_type": report_type,
            "report_type_name": REPORT_TYPE_NAMES.get(report_type, "æœªçŸ¥"),
            "chunk_size": chunk_size,
            "total_chunks": len(chunk_stats),
            "total_rows": total_rows,
            "total_cleaned_rows": total_cleaned_rows,
            "total_errors": len(all_errors),
            "total_warnings": len(all_warnings),
            "elapsed_time_ms": round(elapsed_time * 1000, 2),
            "throughput_rows_per_sec": (
                round(total_rows / elapsed_time, 2) if elapsed_time > 0 else 0
            ),
        }

        print("\nğŸ“‹ å¤„ç†æ±‡æ€»:")
        print(to_json(summary_info))

        # é”™è¯¯é¢„è§ˆï¼ˆå‰ 5 ä¸ªï¼‰
        if all_errors:
            print("\nğŸ“‹ é”™è¯¯é¢„è§ˆ (å‰ 5 ä¸ª):")
            error_preview = [
                {
                    "row_index": err.row_index,
                    "column": err.column,
                    "error_type": (
                        err.error_type.value
                        if hasattr(err.error_type, "value")
                        else str(err.error_type)
                    ),
                    "message": err.message,
                }
                for err in all_errors[:5]
            ]
            print(to_json(error_preview))

            if len(all_errors) > 5:
                print(f"   ... è¿˜æœ‰ {len(all_errors) - 5} ä¸ªé”™è¯¯æœªæ˜¾ç¤º")

        # è­¦å‘Šé¢„è§ˆï¼ˆå‰ 3 ä¸ªï¼‰
        if all_warnings:
            print("\nğŸ“‹ è­¦å‘Šé¢„è§ˆ (å‰ 3 ä¸ª):")
            warning_preview = [
                {
                    "row_index": err.row_index,
                    "column": err.column,
                    "message": err.message,
                }
                for err in all_warnings[:3]
            ]
            print(to_json(warning_preview))

            if len(all_warnings) > 3:
                print(f"   ... è¿˜æœ‰ {len(all_warnings) - 3} ä¸ªè­¦å‘Šæœªæ˜¾ç¤º")

        # æœ€ç»ˆçŠ¶æ€
        print("\n" + "-" * 40)
        if len(all_errors) == 0:
            print(
                f"âœ… å¤„ç†å®Œæˆã€‚æ€»è¡Œæ•°: {total_rows}, æ¸…æ´—å: {total_cleaned_rows}, æ— é”™è¯¯"
            )
        else:
            print(
                f"âš ï¸ å¤„ç†å®Œæˆã€‚æ€»è¡Œæ•°: {total_rows}, æ¸…æ´—å: {total_cleaned_rows}, "
                f"é”™è¯¯: {len(all_errors)}, è­¦å‘Š: {len(all_warnings)}"
            )
        print(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time * 1000:.2f} ms")
        print("-" * 40)

        return True

    except Exception as e:
        # å¼‚å¸¸å¤„ç†ï¼šæ‰“å°é”™è¯¯å †æ ˆ
        elapsed_time = time.time() - start_time
        print(f"\nâŒ æµå¼å¤„ç†å¤±è´¥: {e}")
        print("\nğŸ“‹ é”™è¯¯å †æ ˆ:")
        traceback.print_exc()
        print(f"\nâ±ï¸ å¤„ç†è€—æ—¶ (å¤±è´¥): {elapsed_time * 1000:.2f} ms")

        return False


def process_file(
    file_path: str,
    stream_mode: bool = False,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    auto_stream_threshold: int = LARGE_FILE_THRESHOLD,
) -> bool:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆè‡ªåŠ¨é€‰æ‹©å…¨é‡æˆ–æµå¼æ¨¡å¼ï¼‰

    å†³ç­–é€»è¾‘ï¼š
    1. å¦‚æœ stream_mode=True ä¸”ä¸º CSV æ–‡ä»¶ï¼Œä½¿ç”¨æµå¼å¤„ç†
    2. å¦‚æœæ–‡ä»¶å¤§å° > auto_stream_threshold ä¸”ä¸º CSV æ–‡ä»¶ï¼Œè‡ªåŠ¨å¯ç”¨æµå¼å¤„ç†
    3. å¦åˆ™ä½¿ç”¨å…¨é‡å¤„ç†

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        stream_mode: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨æµå¼æ¨¡å¼
        chunk_size: æµå¼å¤„ç†çš„ chunk å¤§å°
        auto_stream_threshold: è‡ªåŠ¨å¯ç”¨æµå¼å¤„ç†çš„æ–‡ä»¶å¤§å°é˜ˆå€¼ï¼ˆå­—èŠ‚ï¼‰

    Returns:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    file_path_obj = Path(file_path)
    is_csv = file_path_obj.suffix.lower() == ".csv"

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨æµå¼å¤„ç†
    use_stream = False
    if stream_mode and is_csv:
        use_stream = True
    elif is_csv and file_path_obj.exists():
        file_size = file_path_obj.stat().st_size
        if file_size > auto_stream_threshold:
            use_stream = True
            print(
                f"ğŸ’¡ è‡ªåŠ¨å¯ç”¨æµå¼å¤„ç† (æ–‡ä»¶å¤§å° {file_size / (1024*1024):.2f} MB > {auto_stream_threshold / (1024*1024):.0f} MB é˜ˆå€¼)"
            )

    if use_stream:
        return process_file_stream(file_path, chunk_size=chunk_size)
    else:
        return process_file_full(file_path)


def get_default_test_files() -> List[str]:
    """
    è·å–é»˜è®¤æµ‹è¯•æ–‡ä»¶åˆ—è¡¨

    Returns:
        List[str]: æµ‹è¯•æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    # é¡¹ç›®æ ¹ç›®å½•
    project_root = BACKEND_DIR.parent
    docs_dir = project_root / "docs"

    test_files = []

    # ä¼˜å…ˆä½¿ç”¨å®˜æ–¹æ ·ä¾‹ XLSX æ–‡ä»¶
    xlsx_patterns = [
        "é¢„è®¢æ±‡æ€»*.xlsx",
        "é…’æ°´é”€å”®*.xlsx",
        "åŒ…å¢å¼€å°*.xlsx",
    ]

    for pattern in xlsx_patterns:
        matched_files = sorted(docs_dir.glob(pattern))
        for file_path in matched_files:
            test_files.append(str(file_path))
            if len(test_files) >= 3:
                break
        if len(test_files) >= 3:
            break

    # å…œåº•ï¼šä½¿ç”¨æ‰€æœ‰ XLSX æ–‡ä»¶
    if not test_files:
        xlsx_files = sorted(docs_dir.glob("*.xlsx"))
        test_files = [str(f) for f in xlsx_files[:3]]

    # æœ€åå…œåº•ï¼šä½¿ç”¨ CSV æ–‡ä»¶
    if not test_files:
        csv_patterns = [
            "å‰¯æœ¬é¢„è®¢æ±‡æ€».csv",
            "å‰¯æœ¬é…’æ°´é”€å”®åˆ†æ.csv",
            "å‰¯æœ¬åŒ…å¢å¼€å°åˆ†æ.csv",
        ]
        for pattern in csv_patterns:
            file_path = docs_dir / pattern
            if file_path.exists():
                test_files.append(str(file_path))

    return test_files


def parse_arguments() -> argparse.Namespace:
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°

    Returns:
        argparse.Namespace: è§£æåçš„å‚æ•°å¯¹è±¡
    """
    parser = argparse.ArgumentParser(
        description="KTV æŠ¥è¡¨ç³»ç»Ÿ - ETL è”è°ƒè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python main.py                                  # å¤„ç†é»˜è®¤æµ‹è¯•æ–‡ä»¶
  python main.py data1.xlsx data2.csv             # å¤„ç†æŒ‡å®šæ–‡ä»¶
  python main.py --stream large_data.csv          # å¼ºåˆ¶æµå¼å¤„ç†
  python main.py --stream --chunk-size 2000 *.csv # è‡ªå®šä¹‰ chunk å¤§å°
  python main.py --auto-stream-threshold 10       # è®¾ç½®è‡ªåŠ¨æµå¼é˜ˆå€¼ä¸º 10MB

æ³¨æ„:
  - æµå¼å¤„ç†æ¨¡å¼ (--stream) ä»…æ”¯æŒ CSV æ–‡ä»¶
  - Excel æ–‡ä»¶ç”±äºæ ¼å¼é™åˆ¶ï¼Œå§‹ç»ˆä½¿ç”¨å…¨é‡å¤„ç†æ¨¡å¼
  - å½“ CSV æ–‡ä»¶å¤§å°è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œä¼šè‡ªåŠ¨å¯ç”¨æµå¼å¤„ç†
        """,
    )

    parser.add_argument(
        "files",
        nargs="*",
        help="è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡ä»¶ï¼‰",
    )

    parser.add_argument(
        "--stream",
        action="store_true",
        help="å¼ºåˆ¶å¯ç”¨æµå¼å¤„ç†æ¨¡å¼ï¼ˆä»…é™ CSV æ–‡ä»¶ï¼‰",
    )

    parser.add_argument(
        "--chunk-size",
        type=int,
        default=DEFAULT_CHUNK_SIZE,
        metavar="N",
        help=f"æµå¼å¤„ç†æ—¶æ¯ä¸ª chunk çš„è¡Œæ•°ï¼ˆé»˜è®¤: {DEFAULT_CHUNK_SIZE}ï¼‰",
    )

    parser.add_argument(
        "--auto-stream-threshold",
        type=float,
        default=LARGE_FILE_THRESHOLD / (1024 * 1024),
        metavar="MB",
        help=f"è‡ªåŠ¨å¯ç”¨æµå¼å¤„ç†çš„æ–‡ä»¶å¤§å°é˜ˆå€¼ï¼ˆMBï¼Œé»˜è®¤: {LARGE_FILE_THRESHOLD / (1024 * 1024):.0f}ï¼‰",
    )

    parser.add_argument(
        "--no-auto-stream",
        action="store_true",
        help="ç¦ç”¨è‡ªåŠ¨æµå¼å¤„ç†ï¼ˆå³ä½¿å¤§æ–‡ä»¶ä¹Ÿä½¿ç”¨å…¨é‡æ¨¡å¼ï¼‰",
    )

    return parser.parse_args()


def main():
    """
    ä¸»å…¥å£å‡½æ•°
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    print("\n" + "=" * 70)
    print("ğŸš€ KTV æŠ¥è¡¨ç³»ç»Ÿ - ETL è”è°ƒè„šæœ¬")
    print("=" * 70)
    print("åŠŸèƒ½: éªŒè¯ Parser -> Cleaner å®Œæ•´é“¾è·¯")
    print("è¾“å‡º: JSON æ ¼å¼çš„æ¸…æ´—æ•°æ®å’Œæ ¡éªŒæŠ¥å‘Š")

    # æ˜¾ç¤ºè¿è¡Œæ¨¡å¼
    if args.stream:
        print(f"æ¨¡å¼: æµå¼å¤„ç† (å¼ºåˆ¶), Chunk å¤§å°: {args.chunk_size} è¡Œ")
    elif args.no_auto_stream:
        print("æ¨¡å¼: å…¨é‡å¤„ç† (ç¦ç”¨è‡ªåŠ¨æµå¼)")
    else:
        print(
            f"æ¨¡å¼: è‡ªåŠ¨ (å¤§äº {args.auto_stream_threshold:.0f}MB çš„ CSV æ–‡ä»¶å¯ç”¨æµå¼å¤„ç†)"
        )

    print("=" * 70)

    # è·å–å¾…å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if args.files:
        files_to_process = args.files
        print(f"\nğŸ“ å¾…å¤„ç†æ–‡ä»¶ (å‘½ä»¤è¡ŒæŒ‡å®š): {len(files_to_process)} ä¸ª")
    else:
        # ä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡ä»¶
        files_to_process = get_default_test_files()
        if not files_to_process:
            print("\nâš ï¸ æœªæ‰¾åˆ°é»˜è®¤æµ‹è¯•æ–‡ä»¶ï¼Œè¯·é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ–‡ä»¶:")
            print("   python main.py <file1> <file2> ...")
            print("   python main.py --help  # æŸ¥çœ‹å¸®åŠ©")
            return
        print(f"\nğŸ“ å¾…å¤„ç†æ–‡ä»¶ (é»˜è®¤): {len(files_to_process)} ä¸ª")

    for file_path in files_to_process:
        print(f"   - {file_path}")

    # è®¡ç®—è‡ªåŠ¨æµå¼å¤„ç†é˜ˆå€¼ï¼ˆè½¬æ¢ä¸ºå­—èŠ‚ï¼‰
    auto_threshold = int(args.auto_stream_threshold * 1024 * 1024)
    if args.no_auto_stream:
        # ç¦ç”¨è‡ªåŠ¨æµå¼å¤„ç†ï¼šè®¾ç½®ä¸€ä¸ªæå¤§çš„é˜ˆå€¼
        auto_threshold = float("inf")

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    success_count = 0
    fail_count = 0

    for file_path in files_to_process:
        try:
            success = process_file(
                file_path,
                stream_mode=args.stream,
                chunk_size=args.chunk_size,
                auto_stream_threshold=auto_threshold,
            )
            if success:
                success_count += 1
            else:
                fail_count += 1
        except Exception as e:
            print(f"\nâŒ æ–‡ä»¶å¤„ç†å¼‚å¸¸: {file_path}")
            traceback.print_exc()
            fail_count += 1

    # è¾“å‡ºæ±‡æ€»
    print("\n" + "=" * 70)
    print("ğŸ“Š å¤„ç†æ±‡æ€»")
    print("=" * 70)
    print(f"   æ€»æ–‡ä»¶æ•°: {len(files_to_process)}")
    print(f"   æˆåŠŸ: {success_count}")
    print(f"   å¤±è´¥: {fail_count}")
    print("=" * 70)
    print("\nâœ… ETL è”è°ƒè„šæœ¬æ‰§è¡Œå®Œæ¯•\n")


# ============================================================================
# è„šæœ¬å…¥å£
# ============================================================================

if __name__ == "__main__":
    main()
